Для каждого сайта я сделал собственный парсер и ноутбук с анализом данных. Они лежат в соответсвующих папках.
Для сайта https://eabr.org/ получилось выполнить задание полностью.
Для сайта https://www.bangkokpost.com/ у меня не получилось собрать данные полноценным парсером, так как сайт замедлял работу краулера, поэтому я собрал данные с
помощью упрощенной версии, где брал инорфмацию только из архива. Но функция и фрагмент кода для сбора данных с страницы каждой новости реализованы и работают.
Для сайта https://asean.org/ не полуичлось написать краулер, так как сайт достаточно быстро блокировал, а в архиве по новстям недостаточно информации.